{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1dd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Code Starting**\n",
      "**End of Importing**\n"
     ]
    }
   ],
   "source": [
    "print('**Code Starting**')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from leafvein2 import Leafvein\n",
    "\n",
    "from backboneModels import *\n",
    "from utils import *\n",
    "\n",
    "print('**End of Importing**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cb94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import DeformConv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bff512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reproducility\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb9c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc=[]\n",
    "train_acc=[]\n",
    "train_total_losses=[]\n",
    "test_total_losses=[]\n",
    "train_iou=[]\n",
    "test_iou=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0327a008",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3550541012.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    parser.add_argument('--backbone_class', type=str, default='densenet161', choices=['densenet161', 'densenet121',  'resnet50', 'resnet34', 'resnet18', 'mobilenet_v2', 'mobilenet_v3_large' ],'Backbone models')\u001b[0m\n\u001b[0m                                                                                                                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description='Training PyTorch Models For Ultra Fine Grained')\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--lr', default=0.05, type=float, help='learning rate')\n",
    "parser.add_argument('--max_epoch', default=150, type=int)\n",
    "parser.add_argument('--backbone_class', type=str, default='densenet161', choices=['densenet161',  'resnet50', 'resnet34', 'resnet18', 'mobilenet_v2', 'mobilenet_v3_large' ], help='Backbone models')\n",
    "parser.add_argument('--dataset', type=str, default='soybean_2_1', choices=['soybean_2_1', 'btf', 'hainan_leaf'], help='resume from checkpoint')\n",
    "parser.add_argument('--data_dir', type=str, default='./data')\n",
    "parser.add_argument('--seg_size', default=448, type=int, help='Segmentation Dimension')\n",
    "parser.add_argument('--num_classes', default=200, type=int, help='Number of Classes')\n",
    "\n",
    "parser.add_argument('--dataparallel', action='store_true', help='Enable Data Parallel')\n",
    "parser.add_argument('--seg_ild', action='store_true', help='Enable Segmentation Training')\n",
    "parser.add_argument('--cls_ild', action='store_true', help='Enable Classification Training')\n",
    "parser.add_argument('--freeze_all', action='store_true', help='Freeze the Encoder Module')\n",
    "parser.add_argument('--manet', action='store_true', help='Using the MANet')\n",
    "parser.add_argument('--mmanet', action='store_true', help='Using the MMANet')\n",
    "parser.add_argument('--maskguided', action='store_true',help='Guiding the Attention Mask')\n",
    "parser.add_argument('--unet', action='store_true', help='Unet based Segmentation, Unet3+ otherwise')\n",
    "parser.add_argument('--deform_expan', default=3,type=float, help='Applying mean attention to Encoder outputs')\n",
    "\n",
    "parser.add_argument('--model_path', type=str, help='The pretrained model path')\n",
    "parser.add_argument('--fsds', action='store_true', help='Using Full-scale Deep Supervision')\n",
    "\n",
    "parser.add_argument('--local_train', default= 0 , type=int, help='local_training')\n",
    "\n",
    "\n",
    "\n",
    "# Use parse_known_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print('args.local_train',args.local_train)\n",
    "\n",
    "\n",
    "if args.model_path is not None:\n",
    "    args.batch_size=8\n",
    "    if not args.unet:\n",
    "        args.lr=0.02\n",
    "else:\n",
    "    args.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f56ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bf6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/Original_14:19-15-12-2023.pth\n"
     ]
    }
   ],
   "source": [
    "batchsize = args.batch_size\n",
    "\n",
    "\n",
    "MANet=args.manet\n",
    "MMANet=args.mmanet\n",
    "mask_guided=args.maskguided\n",
    "\n",
    "seg_ild=args.seg_ild\n",
    "cls_ild=args.cls_ild\n",
    "freeze_all=args.freeze_all\n",
    "\n",
    "num_classes=args.num_classes\n",
    "\n",
    "model_name=args.backbone_class\n",
    "\n",
    "start_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee2a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches\n",
      "folder_path Original\n",
      "/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches/results/resnet50/Original/14:19-15-12-2023/model_accuracies.txt\n"
     ]
    }
   ],
   "source": [
    "#current_working_directory = os.getcwd()\n",
    "if args.local_train==1:\n",
    "    current_working_directory = '/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches_AMP/'\n",
    "else:\n",
    "    current_working_directory = '/mnt/mywork/all_backbones/'\n",
    "print(\"Current Working Directory:\", current_working_directory)\n",
    "\n",
    "name=get_folder_path(args)\n",
    "print('folder_path',name)\n",
    "\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format the hour:minutes date and time as day-month-year \n",
    "formatted_datetime = current_datetime.strftime(\"%H:%M-%d-%m-%Y\")\n",
    "\n",
    "if cls_ild and not(seg_ild):\n",
    "    train_type=args.dataset+'-results-cls'\n",
    "\n",
    "elif seg_ild and not (cls_ild):\n",
    "    if args.unet:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg','Unet'+str(args.deform_expan))\n",
    "    else:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg','Unet3Plus'+str(args.deform_expan))\n",
    "\n",
    "else:\n",
    "    if args.unet:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg-cls','Unet'+str(args.deform_expan))\n",
    "    else:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg-cls','Unet3Plus'+str(args.deform_expan))\n",
    "\n",
    "folder_path =os.path.join(current_working_directory,train_type,args.backbone_class,name,formatted_datetime)\n",
    "accuracy_file_path =os.path.join(folder_path,'model_accuracies_iou.txt')\n",
    "    \n",
    "    \n",
    "print('File path:',accuracy_file_path)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "\n",
    "models_folder=current_working_directory+'/checkpoint'\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad4b836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch_size 32\n",
      "MMANet False\n",
      "MGANet False\n",
      "mask_guided False\n",
      "seg_included False\n",
      "cls_included False\n",
      "freeze_all False\n"
     ]
    }
   ],
   "source": [
    "print('\\nBatch_size',args.batch_size)\n",
    "\n",
    "print('MANet',MANet)\n",
    "print('MMANet',MMANet)\n",
    "print('mask_guided',mask_guided)\n",
    "\n",
    "print('seg_included',seg_ild)\n",
    "print('cls_included',cls_ild)\n",
    "\n",
    "print('freeze_all',freeze_all)\n",
    "print('Full-scale Deep Supervision',args.fsds)\n",
    "print('Unet',args.unet)\n",
    "print('deform_expan',args.deform_expan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "train = Leafvein(args,crop=[448,448],hflip=True,vflip=False,erase=False,mode='train')\n",
    "test = Leafvein(args,crop=[448,448],mode='test')\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce605cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MMANET(backbone_name=model_name,num_classes=num_classes,MANet=MANet,MMANet=MMANet,mask_guided=mask_guided,seg_included=seg_ild,freeze_all=freeze_all,Unet=args.unet,deform_expan=args.deform_expan)\n",
    "\n",
    "\n",
    "if args.model_path is not None:\n",
    "    print('Loading weights')\n",
    "    model_path= args.model_path\n",
    "    model_dict = torch.load(model_path)\n",
    "    state_dict = model_dict['net'] \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "else:\n",
    "    model_path= f'{current_working_directory}/checkpoint/{name}_{formatted_datetime}.pth'\n",
    "\n",
    "    \n",
    "print('model.seg_included',model.seg_included)\n",
    "print('model.MMANet',model.MMANet)\n",
    "print('model.MANet',model.MANet)    \n",
    "\n",
    "\n",
    "\n",
    "net=model.to(device)\n",
    "\n",
    "if device == 'cuda' and args.dataparallel:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "if args.seg_ild:\n",
    "    model_path= f'{current_working_directory}/checkpoint/{name}-Segmentation-{formatted_datetime}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4fdba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in the model: {total_params/1e+6}\")\n",
    "\n",
    "\n",
    "with open(accuracy_file_path, 'a') as f:\n",
    "    f.write(f'\\n ***************************Start******************************** \\n')\n",
    "    for arg in vars(args):\n",
    "        f.write(f\"{arg}: {getattr(args, arg)}  \\n\")\n",
    "    f.write(f\"Total parameters in the model: {total_params/1e+6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab33a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1,alpha2,alpha3,alpha4,alpha5=0.95, 0.1/4, 0.1/8, 0.1/16, 0.1/32\n",
    "\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "seg_loss_fn   = nn.BCEWithLogitsLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437bf44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [args.seg_size, args.seg_size]\n",
    "new_size = [size // 2 for size in input_size]\n",
    "\n",
    "def train_epoch_Seg(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    global new_size\n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'\\n Epoch:{epoch}\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    if freeze_all and not(cls_ild):\n",
    "    \n",
    "        for name, module in net.module.features.named_modules():\n",
    "            if isinstance(module, (nn.BatchNorm2d, nn.Dropout)):\n",
    "                module.eval()\n",
    "                \n",
    "                \n",
    "    train_loss = 0\n",
    "    train_ce_loss=0\n",
    "    averageIoU=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    for batch_idx, (inputs, targets, masks) in enumerate(trainloader):\n",
    "        inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "\n",
    "        masks=masks.unsqueeze(1)\n",
    "        masks = F.interpolate(masks, size=new_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        ce_loss_,se_loss_,mse_loss_=0,0,0\n",
    "        \n",
    "        \n",
    "        if mask_guided:\n",
    "            outputs = net(inputs, masks) \n",
    "            msks=outputs['mask']\n",
    "            fg_att=outputs['fg_att']\n",
    "            mse_loss_ = mse(fg_att,msks)\n",
    "\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        if seg_ild:\n",
    "            Final_seg=outputs['Final_seg']\n",
    "            se_loss_ = seg_loss_fn(Final_seg,masks)\n",
    "\n",
    "            preds = torch.sigmoid(Final_seg)\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "            iou = iou_binary(preds, masks)\n",
    "            averageIoU+=iou\n",
    "\n",
    "            if args.fsds:\n",
    "\n",
    "\n",
    "                lvl_2_loss = seg_loss_fn(outputs['decoder_layer_2'],masks)\n",
    "\n",
    "                fsds_size = [size // 2 for size in new_size]\n",
    "                masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                lvl_3_loss = seg_loss_fn(outputs['decoder_layer_3'],masks)\n",
    "\n",
    "                fsds_size = [size // 2 for size in fsds_size]\n",
    "                masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                lvl_4_loss = seg_loss_fn(outputs['decoder_layer_4'],masks)\n",
    "\n",
    "                fsds_size = [size // 2 for size in fsds_size]\n",
    "                masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                lvl_5_loss = seg_loss_fn(outputs['decoder_layer_5'],masks)\n",
    "                se_loss_= alpha1*se_loss_+ alpha2*lvl_2_loss+ alpha3*lvl_3_loss+ alpha4*lvl_4_loss + alpha5*lvl_5_loss\n",
    "\n",
    "\n",
    "\n",
    "            out=outputs['out']\n",
    "            ce_loss_ = class_loss_fn(out, targets)\n",
    "\n",
    "            loss =  seg_ild*se_loss_ + cls_ild*ce_loss_+ mask_guided*0.1*mse_loss_\n",
    "\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_ce_loss+=ce_loss_.item()\n",
    "\n",
    "        \n",
    "        _, predicted = out.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        Final_seg,masks,inputs,targets,outputs=None,None,None,None,None\n",
    "    \n",
    "    train_ce_loss/=(batch_idx+1)\n",
    "    train_loss/=(batch_idx+1)\n",
    "        \n",
    "    averageIoU=averageIoU*100/(batch_idx+1)\n",
    "    accuracy=100.*correct/total\n",
    "    \n",
    "    print(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {train_ce_loss:.4f}| Total Loss: {train_loss:.4f}| IoU :{averageIoU:.4f}')\n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {train_ce_loss:.4f}| Total Loss: {train_loss:.4f}| IoU :{averageIoU:.4f}\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    train_total_losses.append(train_loss)\n",
    "    train_iou.append(averageIoU)\n",
    "    return averageIoU,accuracy,train_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch_Seg(epoch):\n",
    "    net.eval()\n",
    "    global best_iou, best_acc, new_size\n",
    "    test_loss = 0\n",
    "    test_ce_loss = 0\n",
    "    \n",
    "    averageIoU=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets,masks) in enumerate(testloader):\n",
    "            inputs, targets,masks= inputs.to(device), targets.to(device),masks.to(device)\n",
    "            \n",
    "            \n",
    "            masks=masks.unsqueeze(1)\n",
    "            masks = F.interpolate(masks, size=new_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "\n",
    "            ce_loss_,se_loss_,mse_loss_=0,0,0\n",
    "\n",
    "            if mask_guided:\n",
    "                outputs=net(inputs,masks)\n",
    "                msks=outputs['mask']\n",
    "                fg_att=outputs['fg_att']\n",
    "                mse_loss_ = mse(fg_att,msks)\n",
    "            else:\n",
    "                outputs=net(inputs)\n",
    "                \n",
    "\n",
    "            if seg_ild:\n",
    "                Final_seg=outputs['Final_seg']\n",
    "                se_loss_ = seg_loss_fn(Final_seg,masks)\n",
    "\n",
    "                preds = torch.sigmoid(Final_seg)\n",
    "                preds = (preds > 0.5).float()\n",
    "\n",
    "                iou = iou_binary(preds, masks)\n",
    "                averageIoU+=iou\n",
    "                \n",
    "                                    \n",
    "                if args.fsds:\n",
    "                    \n",
    "                    \n",
    "                    lvl_2_loss = seg_loss_fn(outputs['decoder_layer_2'],masks)\n",
    "\n",
    "                    fsds_size = [size // 2 for size in new_size]\n",
    "                    masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                    lvl_3_loss = seg_loss_fn(outputs['decoder_layer_3'],masks)\n",
    "\n",
    "                    fsds_size = [size // 2 for size in fsds_size]\n",
    "                    masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                    lvl_4_loss = seg_loss_fn(outputs['decoder_layer_4'],masks)\n",
    "\n",
    "                    fsds_size = [size // 2 for size in fsds_size]\n",
    "                    masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                    lvl_5_loss = seg_loss_fn(outputs['decoder_layer_5'],masks)\n",
    "                    se_loss_= alpha1*se_loss_+ alpha2*lvl_2_loss+ alpha3*lvl_3_loss+ alpha4*lvl_4_loss + alpha5*lvl_5_loss\n",
    "\n",
    "\n",
    "            out=outputs['out']\n",
    "            ce_loss_ = class_loss_fn(out, targets)\n",
    "\n",
    "            loss =  seg_ild*se_loss_ + cls_ild*ce_loss_+ mask_guided*0.1*mse_loss_\n",
    "\n",
    "\n",
    "\n",
    "            test_ce_loss+=ce_loss_.item()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _, predicted = out.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            segs,masks,inputs,targets,outputs=None,None,None,None,None\n",
    "\n",
    "    averageIoU=averageIoU*100/(batch_idx+1)\n",
    "    accuracy=100.*correct/total\n",
    "    \n",
    "\n",
    "    if averageIoU>best_iou:\n",
    "        best_iou=averageIoU\n",
    "        \n",
    "    if accuracy>best_acc:\n",
    "        best_acc=accuracy\n",
    "    \n",
    "    test_ce_loss/=(batch_idx+1)\n",
    "    test_loss/=(batch_idx+1)\n",
    "    \n",
    "    test_total_losses.append(test_loss)\n",
    "    test_iou.append(averageIoU)\n",
    "    \n",
    "    print(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {test_ce_loss:.4f}|  Total Loss: {test_loss:.4f}| IoU :{averageIoU:.4f}')\n",
    "    print('cur_iou:{0},best_iou:{1}:'.format(averageIoU,best_iou))\n",
    "    print('curr_Acc:{0},best_Acc:{1}:'.format(accuracy,best_acc))\n",
    "    \n",
    "    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {test_ce_loss:.4f}| Total Loss: {test_loss:.4f}| IoU :{averageIoU:.4f}\\n')\n",
    "        f.write(f'cur_iou:{averageIoU},best_iou:{best_iou}\\n')\n",
    "        f.write(f'curr_Acc:{accuracy},best_Acc:{best_acc}\\n')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    return averageIoU,accuracy,test_ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seg_performance(iou,curr_test_iou,test_acc,net,epoch,end,start,model_path,accuracy_file_path):\n",
    "    global train_best_iou,best_test_iou,Best_test_acc_BOT\n",
    "    if iou>train_best_iou:\n",
    "        print('Saving..')\n",
    "        train_best_iou=iou\n",
    "        best_test_iou=curr_test_iou\n",
    "        Best_test_acc_BOT=test_acc\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net_wrap = net.module\n",
    "        else:\n",
    "            net_wrap=net\n",
    "        state = {'net': net_wrap.state_dict(), 'test_iou': best_test_iou, 'Test_acc': test_acc , 'epoch': epoch,}\n",
    "        torch.save(state, model_path)\n",
    "    print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "    print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "    print(f'Time Elapsed:{end-start}\\n')    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "        f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "        f.write(f'Time Elapsed:{end-start}\\n')\n",
    "\n",
    "\n",
    "            \n",
    "def check_class_performance(train_ce_loss, test_acc, curr_test_iou,net, epoch, model_path, accuracy_file_path, start, end):    \n",
    "    global min_loss, Best_test_acc_BOT, best_test_iou\n",
    "    if train_ce_loss<min_loss:\n",
    "        print('Saving..')\n",
    "        min_loss=train_ce_loss\n",
    "        Best_test_acc_BOT=test_acc\n",
    "        best_test_iou=curr_test_iou\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net_nwrap = net.module\n",
    "        else:\n",
    "            net_nwrap=net\n",
    "        state = {'net': net_nwrap.state_dict(), 'test_iou': curr_test_iou, 'Test_acc': test_acc, 'epoch': epoch,}\n",
    "        torch.save(state, model_path)\n",
    "    \n",
    "    print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "    print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "    print(f'Time Elapsed:{end-start}\\n')    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "        f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "        f.write(f'Time Elapsed:{end-start}\\n')\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "def check_performance(iou, curr_test_iou, train_ce_loss, test_acc, net, epoch, model_path, accuracy_file_path, start, end):\n",
    "    global train_best_iou, best_test_iou, Best_test_acc_BOT, min_loss\n",
    "    updated = False\n",
    "    # Check for segment performance improvement\n",
    "    if iou > train_best_iou:\n",
    "        print('Saving based on segment performance improvement..')\n",
    "        train_best_iou = iou\n",
    "        best_test_iou = curr_test_iou\n",
    "        Best_test_acc_BOT = test_acc\n",
    "        updated = True\n",
    "\n",
    "    # Check for class performance improvement\n",
    "    if train_ce_loss < min_loss:\n",
    "        print('Saving based on class performance improvement..')\n",
    "        min_loss = train_ce_loss\n",
    "        Best_test_acc_BOT = test_acc\n",
    "        best_test_iou = curr_test_iou\n",
    "        updated = True\n",
    "\n",
    "    # Save the model if there was an update\n",
    "    if updated:\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net_nwrap = net.module\n",
    "        else:\n",
    "            net_nwrap = net\n",
    "        state = {'net': net_nwrap.state_dict(),'test_iou': curr_test_iou,'test_acc': test_acc,'epoch': epoch,}\n",
    "        torch.save(state, model_path)\n",
    "        \n",
    "    print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "    print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "    print(f'Time Elapsed:{end-start}\\n')    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "        f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "        f.write(f'Time Elapsed:{end-start}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f1e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_iou=0\n",
    "best_acc=0\n",
    "\n",
    "train_best_iou=0\n",
    "best_test_iou=0\n",
    "\n",
    "min_loss=1e10\n",
    "test_acc=0\n",
    "Best_test_acc_BOT=0\n",
    "\n",
    "for epoch in range(start_epoch, args.max_epoch):\n",
    "    start = time.time()\n",
    "    iou,train_acc,train_ce_loss= train_epoch_Seg(epoch)\n",
    "    curr_test_iou,test_acc,test_ce_loss=test_epoch_Seg(epoch)\n",
    "    scheduler.step()\n",
    "    end = time.time()\n",
    "    \n",
    "    if seg_ild and not (cls_ild):\n",
    "        check_seg_performance(iou,curr_test_iou,test_acc,net,epoch,end,start,model_path,accuracy_file_path) \n",
    "    elif not(seg_ild) and cls_ild:\n",
    "        check_class_performance(train_ce_loss, test_acc, curr_test_iou,net, epoch, model_path, accuracy_file_path, start, end)\n",
    "    else:\n",
    "        check_performance(iou, curr_test_iou, train_ce_loss, test_acc, net, epoch, model_path, accuracy_file_path, start, end)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_file =os.path.join(current_working_directory,train_type,args.backbone_class,'all_results_file.txt')\n",
    "\n",
    "\n",
    "\n",
    "from filelock import Timeout, FileLock\n",
    "lock = FileLock(all_results_file[:-4]+'.lock', timeout=120)  # Timeout after two minutes\n",
    "try:\n",
    "    with lock:\n",
    "        with open(all_results_file, 'a') as file:\n",
    "            file.write(f'\\n*************************************************************\\n')\n",
    "            file.write(f\"**{name} 's Testing Accuracies**\\n\")\n",
    "            file.write(f'***Total parameters in the model: {total_params/1e+6}***\\n')            \n",
    "            file.write(f'*****{formatted_datetime} Time*****\\n')\n",
    "            file.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "            file.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "            file.write(f'*************************************************************\\n\\n')\n",
    "except Timeout:\n",
    "    print(\"Could not acquire the lock within 120 seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9733773",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Add arguments\n",
    "parser.add_argument('--lr', default=0.05, type=float, help='learning rate')\n",
    "parser.add_argument('--max_epoch', default=150, type=int, help='resume from checkpoint')\n",
    "parser.add_argument('--backbone_class', type=str, default='densenet161', choices=['densenet161', 'vgg19', 'resnet50', 'resnet34', 'mobilenet_v2', 'inception_v3'], help='resume from checkpoint')\n",
    "parser.add_argument('--dataset', type=str, default='soybean_2_1', choices=['soybean_1_1', 'soybean_2_1', 'btf', 'hainan_leaf'], help='resume from checkpoint')\n",
    "parser.add_argument('--data_dir', type=str, default='./data')\n",
    "parser.add_argument('--Seg_Size', default=448, type=int, help='Segmentation Dimension')\n",
    "parser.add_argument('--num_classes', default=200, type=int, help='Number of Classes')\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('--DataParallel', action='store_true', help='Enable Data Parallel')\n",
    "parser.add_argument('--seg_included', action='store_true', help='Enable Segmentation Training')\n",
    "parser.add_argument('--cls_included', action='store_true', help='Enable Classification Training')\n",
    "parser.add_argument('--freeze_all', action='store_true', help='Freeze the Encoder Module')\n",
    "parser.add_argument('--MMANet', action='store_true', help='Using the MMANet')\n",
    "parser.add_argument('--MGANet', action='store_true', help='Using the MGANet')\n",
    "parser.add_argument('--maskguided', action='store_true', help='Using the MGANet')\n",
    "parser.add_argument('--Unet', action='store_true', help='Using the Unet')\n",
    "parser.add_argument('--mp', action='store_true', help='using automatic mixed precision training”')\n",
    "\n",
    "parser.add_argument('--model_path', type=str , help='Use Pretrained Model')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567167b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch: 0\n",
    "Acc: 0.375% (3/800)| CE: 5.5615| Total Loss: 5.5615| IoU :0.0000\n",
    "Acc: 0.000% (0/400)| CE: 6.0858|  Total Loss: 6.0858| IoU :0.0000\n",
    "cur_iou:0.0,best_iou:0:\n",
    "curr_Acc:0.0,best_Acc:0:\n",
    "Saving..\n",
    "Best Testing IoU Based On the Training:0.0\n",
    "Best Testing Accuracy Based On the Training:0.0\n",
    "Time Elapsed:20.380533456802368\n",
    "\n",
    "\n",
    "Epoch: 1\n",
    "Acc: 0.875% (7/800)| CE: 5.1327| Total Loss: 5.1327| IoU :0.0000\n",
    "Acc: 3.500% (14/400)| CE: 4.9119|  Total Loss: 4.9119| IoU :0.0000\n",
    "cur_iou:0.0,best_iou:0:\n",
    "curr_Acc:3.5,best_Acc:3.5:\n",
    "Saving..\n",
    "Best Testing IoU Based On the Training:0.0\n",
    "Best Testing Accuracy Based On the \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ac7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from copy import deepcopy\n",
    "# net_original = deepcopy(net)\n",
    "\n",
    "\n",
    "# x = torch.randn(8, 1, 28, 28)\n",
    "\n",
    "# # Define a 2x2 max pooling layer\n",
    "# pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# # Apply the pooling layer to the tensor\n",
    "# y = pool(x)\n",
    "\n",
    "# print(y.size())\n",
    "\n",
    "# def compare_model_params(model1, model2):\n",
    "#     # Get state dictionaries\n",
    "#     state_dict1 = model1.state_dict()\n",
    "#     state_dict2 = model2.state_dict()\n",
    "    \n",
    "#     total_unchanged=0\n",
    "#     total_changed=0\n",
    "\n",
    "#     # Compare sizes and if sizes match, compare values\n",
    "#     for ((key1, param1), (key2, param2)) in zip(state_dict1.items(), state_dict2.items()):\n",
    "        \n",
    "#         if param1.size() != param2.size():\n",
    "#             print(f\"Mismatch in size for layer {key1}: {param1.size()} vs {param2.size()}\")\n",
    "#             continue  # Skip further comparison if sizes differ\n",
    "#         # Check if the parameters are the same\n",
    "#         if torch.equal(param1, param2):\n",
    "#             total_unchanged+=1\n",
    "\n",
    "#         else:\n",
    "#             total_changed+=1\n",
    "#             print(f\"Parameters of layer {key1} differ.\")\n",
    "#     print('Changed, unchanged, Both',total_changed,total_unchanged,total_changed+total_unchanged)\n",
    "\n",
    "# compare_model_params(net,net_original)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(train_iou)\n",
    "# plt.show()\n",
    "# plt.plot(test_iou)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print(test_iou[-1])\n",
    "# print(train_iou[-1])\n",
    "# test_iou[250]\n",
    "# print(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84e6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _check_layers(model, upto=None):\n",
    "#     cnt, th = 0, 0\n",
    "#     print('freeze layers:')\n",
    "#     if upto:\n",
    "#         th = upto\n",
    "#     else:\n",
    "#         th=float('inf')\n",
    "#     for name, child in model.named_children():\n",
    "#         cnt += 1\n",
    "#         if cnt < th:\n",
    "#             layer_type = type(child).__name__\n",
    "#             for name2, params in child.named_parameters():\n",
    "#                 #layer_type2 = type(params).__name__\n",
    "                \n",
    "#                 print(name, layer_type,name2, cnt,params.requires_grad) \n",
    "# #                 if params.requires_grad==True:\n",
    "# #                     number_of_unfrozen_layers+=1\n",
    "\n",
    "\n",
    "\n",
    "# # def _check_layers(model, prefix='', cnt=0, th=float('inf')):\n",
    "# #     if cnt >= th:\n",
    "# #         return\n",
    "# #     for name, child in model.named_children():\n",
    "# #         cnt += 1\n",
    "# #         new_prefix = f\"{prefix}{name}.\" if prefix else name\n",
    "# #         layer_type = type(child).__name__\n",
    "# #         if len(list(child.children())) > 0:  # Check if child has further sub-layers\n",
    "# #             _check_layers(child, new_prefix, cnt, th)\n",
    "# #         for name2, params in child.named_parameters(recurse=False):\n",
    "# #             print(f\"{new_prefix} ({layer_type}) {name2} {cnt} {params.requires_grad}\")\n",
    "\n",
    "\n",
    "\n",
    "# number_of_unfrozen_layers=0\n",
    "\n",
    "# print('*******Features****************')\n",
    "  \n",
    "        \n",
    "# _check_layers(model)\n",
    "\n",
    "\n",
    "# # print('*******Classifier****************')\n",
    "\n",
    "# # nnn=model.classifier\n",
    "# # _check_layers(nnn)\n",
    "\n",
    "\n",
    "\n",
    "# # print('*******Classifier****************')\n",
    "# # for name, param in nnn.named_parameters():\n",
    "# #     print(name,param.requires_grad)\n",
    "# #     if param.requires_grad==True:\n",
    "# #         number_of_unfrozen_layers+=1\n",
    "    \n",
    "    \n",
    "# # print('**********Attention*************')\n",
    "\n",
    "    \n",
    "# # nnn=model.attention\n",
    "# # for name, param in nnn.named_parameters():\n",
    "# #     print(name,param.requires_grad)\n",
    "# #     if param.requires_grad==True:\n",
    "# #         number_of_unfrozen_layers+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "# # print('**********Encoders1*************')\n",
    "\n",
    "# # for i in range (1,6):\n",
    "# #     nnn=model.Encoders[i]\n",
    "# #     _check_layers(nnn)\n",
    "\n",
    "# # print(f'Total unfrozzen:{number_of_unfrozen_layers} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-UnetCRF2] *",
   "language": "python",
   "name": "conda-env-.conda-UnetCRF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
