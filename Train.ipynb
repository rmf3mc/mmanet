{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1dd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Code Starting**\n",
      "**End of Importing**\n"
     ]
    }
   ],
   "source": [
    "print('**Code Starting**')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from   torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from leafvein2 import Leafvein\n",
    "\n",
    "from backboneModels import *\n",
    "from utils import *\n",
    "\n",
    "from MyModelSpace import MyModelSpace\n",
    "\n",
    "print('**End of Importing**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf8c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bff512",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reproducility\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb9c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc=[]\n",
    "train_acc=[]\n",
    "train_total_losses=[]\n",
    "test_total_losses=[]\n",
    "train_iou=[]\n",
    "test_iou=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0327a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.local_train 0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Create the parser\n",
    "parser = argparse.ArgumentParser(description='Training PyTorch Models For Ultra Fine Grained')\n",
    "\n",
    "# Add arguments\n",
    "parser.add_argument('--lr', default=0.05, type=float, help='learning rate')\n",
    "parser.add_argument('--max_epoch', default=150, type=int)\n",
    "parser.add_argument('--backbone_class', type=str, default='densenet161', choices=['densenet161',  'resnet50', 'resnet34', 'resnet18', 'mobilenet_v2', 'mobilenet_v3_large' ], help='Backbone models')\n",
    "parser.add_argument('--dataset', type=str, default='soybean_2_1', choices=['soybean_2_1', 'btf', 'hainan_leaf'], help='resume from checkpoint')\n",
    "parser.add_argument('--data_dir', type=str, default='./data')\n",
    "parser.add_argument('--seg_size', default=448, type=int, help='Segmentation Dimension')\n",
    "parser.add_argument('--num_classes', default=200, type=int, help='Number of Classes')\n",
    "\n",
    "parser.add_argument('--dataparallel', action='store_true', help='Enable Data Parallel')\n",
    "parser.add_argument('--seg_ild', action='store_true', help='Enable Segmentation Training')\n",
    "parser.add_argument('--cls_ild', action='store_true', help='Enable Classification Training')\n",
    "parser.add_argument('--freeze_all', action='store_true', help='Freeze the Encoder Module')\n",
    "parser.add_argument('--manet', action='store_true', help='Using the MANet')\n",
    "parser.add_argument('--mmanet', action='store_true', help='Using the MMANet')\n",
    "parser.add_argument('--maskguided', action='store_true',help='Guiding the Attention Mask')\n",
    "parser.add_argument('--unet', action='store_true', help='Unet based Segmentation, Unet3+ otherwise')\n",
    "parser.add_argument('--deform_expan', default=3,type=float, help='Applying mean attention to Encoder outputs')\n",
    "\n",
    "parser.add_argument('--model_path', type=str, help='The pretrained model path')\n",
    "parser.add_argument('--fsds', action='store_true', help='Using Full-scale Deep Supervision')\n",
    "\n",
    "parser.add_argument('--local_train', default= 0 , type=int, help='local_training')\n",
    "\n",
    "\n",
    "\n",
    "# Use parse_known_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "print('args.local_train',args.local_train)\n",
    "\n",
    "\n",
    "if args.model_path is not None:\n",
    "    args.batch_size=8\n",
    "    if not args.unet:\n",
    "        args.lr=0.02\n",
    "else:\n",
    "    args.batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f56ee7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bf6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = args.batch_size\n",
    "\n",
    "\n",
    "MANet=args.manet\n",
    "MMANet=args.mmanet\n",
    "mask_guided=args.maskguided\n",
    "\n",
    "seg_ild=args.seg_ild\n",
    "cls_ild=args.cls_ild\n",
    "freeze_all=args.freeze_all\n",
    "\n",
    "num_classes=args.num_classes\n",
    "\n",
    "model_name=args.backbone_class\n",
    "\n",
    "start_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aee2a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches_AMP/\n",
      "folder_path soy_densenet161_Original\n",
      "File path: /home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches_AMP/soybean_2_1-results-seg-cls/3PlusUnet/densenet161/soy_densenet161_Original/16:16-17-01-2024/Accuracies_Iou.txt\n"
     ]
    }
   ],
   "source": [
    "if args.local_train==1:\n",
    "    #current_working_directory = os.getcwd()    \n",
    "    current_working_directory = '/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches_AMP/'\n",
    "else:\n",
    "    current_working_directory = '/mnt/mywork/all_backbones/'\n",
    "    \n",
    "current_working_directory = '/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches_AMP/'\n",
    "print(\"Current Working Directory:\", current_working_directory)\n",
    "\n",
    "name=get_folder_path(args)\n",
    "print('folder_path',name)\n",
    "\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Format the hour:minutes date and time as day-month-year \n",
    "formatted_datetime = current_datetime.strftime(\"%H:%M-%d-%m-%Y\")\n",
    "\n",
    "if cls_ild and not(seg_ild):\n",
    "    train_type=args.dataset+'-results-cls'\n",
    "\n",
    "elif seg_ild and not (cls_ild):\n",
    "    if args.unet:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg','Unet')\n",
    "    else:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg','3PlusUnet')\n",
    "\n",
    "else:\n",
    "    if args.unet:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg-cls','Unet')\n",
    "    else:\n",
    "        train_type=os.path.join(args.dataset+'-results-seg-cls','3PlusUnet')\n",
    "\n",
    "folder_path =os.path.join(current_working_directory,train_type,args.backbone_class,name,formatted_datetime)\n",
    "accuracy_file_path =os.path.join(folder_path,'Accuracies_Iou.txt')\n",
    "    \n",
    "    \n",
    "print('File path:',accuracy_file_path)\n",
    "    \n",
    "    \n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    \n",
    "\n",
    "models_folder=current_working_directory+'/checkpoint'\n",
    "if not os.path.exists(models_folder):\n",
    "    os.makedirs(models_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19afbdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wuz8k1j8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-rain-1</strong> at: <a href='https://wandb.ai/ramytrm/icip24-nniexperiment/runs/wuz8k1j8' target=\"_blank\">https://wandb.ai/ramytrm/icip24-nniexperiment/runs/wuz8k1j8</a><br/> View job at <a href='https://wandb.ai/ramytrm/icip24-nniexperiment/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMTMwMDY4MA==/version_details/v0' target=\"_blank\">https://wandb.ai/ramytrm/icip24-nniexperiment/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzMTMwMDY4MA==/version_details/v0</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240117_161547-wuz8k1j8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wuz8k1j8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d79d57a3bd4324840ba8b048b4b07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111287996172905, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pupil/rmf3mc/Documents/ModelProposing/MGANet/FinalTouches/wandb/run-20240117_161645-35or18rt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ramytrm/icip24-nniexperiment/runs/35or18rt' target=\"_blank\">crimson-dust-2</a></strong> to <a href='https://wandb.ai/ramytrm/icip24-nniexperiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ramytrm/icip24-nniexperiment' target=\"_blank\">https://wandb.ai/ramytrm/icip24-nniexperiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ramytrm/icip24-nniexperiment/runs/35or18rt' target=\"_blank\">https://wandb.ai/ramytrm/icip24-nniexperiment/runs/35or18rt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "run=wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"icip24-nniexperiment\",\n",
    "    entity=\"ramytrm\",\n",
    "    group=name,\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": args.lr,\n",
    "    \"backbone_class\": args.backbone_class,\n",
    "    \"dataset\": args.dataset,\n",
    "    \"Max epochs\": args.max_epoch,\n",
    "    \"Segmentation Training\":args.seg_ild,\n",
    "    \"Classification Training\":args.cls_ild,\n",
    "    \"freeze_all\": args.freeze_all,\n",
    "    \"manet\": args.manet,\n",
    "    \"mmanet\": args.mmanet,\n",
    "    \"maskguided\": args.maskguided,\n",
    "    \"unet\": args.unet,\n",
    "    \"deform_expan\": args.deform_expan,\n",
    "    \"fsds\": args.fsds,\n",
    "    \"local_train\": args.local_train\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4b836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch_size 32\n",
      "MANet False\n",
      "MMANet False\n",
      "mask_guided False\n",
      "seg_included False\n",
      "cls_included False\n",
      "freeze_all False\n",
      "Full-scale Deep Supervision False\n",
      "Unet False\n",
      "deform_expan 3\n"
     ]
    }
   ],
   "source": [
    "print('\\nBatch_size',args.batch_size)\n",
    "\n",
    "print('MANet',MANet)\n",
    "print('MMANet',MMANet)\n",
    "print('mask_guided',mask_guided)\n",
    "\n",
    "print('seg_included',seg_ild)\n",
    "print('cls_included',cls_ild)\n",
    "\n",
    "print('freeze_all',freeze_all)\n",
    "print('Full-scale Deep Supervision',args.fsds)\n",
    "print('Unet',args.unet)\n",
    "print('deform_expan',args.deform_expan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74db6e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/soybean_2_1/labels.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mLeafvein\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m448\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m448\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhflip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mvflip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43merase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m Leafvein(args,crop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m448\u001b[39m,\u001b[38;5;241m448\u001b[39m],mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m end\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Documents/ModelProposing/MGANet/FinalTouches/leafvein2.py:31\u001b[0m, in \u001b[0;36mLeafvein.__init__\u001b[0;34m(self, args, crop, hflip, vflip, rotate, erase, mode)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m=\u001b[39mmode\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdataset\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m df:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mload(df)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_files\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/soybean_2_1/labels.pkl'"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "train = Leafvein(args,crop=[448,448],hflip=True,vflip=False,erase=False,mode='train')\n",
    "test = Leafvein(args,crop=[448,448],mode='test')\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12e40629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nni.nas.strategy as strategy\n",
    "search_strategy = strategy.Random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cf702b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatchsize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m testloader \u001b[38;5;241m=\u001b[39m DataLoader(test, batch_size\u001b[38;5;241m=\u001b[39mbatchsize, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader(train, batch_size=batchsize, shuffle=True)\n",
    "testloader = DataLoader(test, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ce605cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.seg_included: False\n",
      "last 2208\n",
      "freeze layers:\n",
      "Freeze layers up to 9s layer\n",
      "model.seg_included False\n",
      "model.MMANet False\n",
      "model.MANet False\n"
     ]
    }
   ],
   "source": [
    "model=MyModelSpace(backbone_name=model_name,num_classes=num_classes,MANet=MANet,MMANet=MMANet,mask_guided=mask_guided,seg_included=seg_ild,freeze_all=freeze_all,Unet=args.unet,deform_expan=args.deform_expan)\n",
    "\n",
    "\n",
    "if args.model_path is not None:\n",
    "    print('Loading weights')\n",
    "    model_path= args.model_path\n",
    "    model_dict = torch.load(model_path)\n",
    "    state_dict = model_dict['net'] \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "else:\n",
    "    model_path= f'{current_working_directory}/checkpoint/{name}_{formatted_datetime}.pth'\n",
    "\n",
    "    \n",
    "print('model.seg_included',model.seg_included)\n",
    "print('model.MMANet',model.MMANet)\n",
    "print('model.MANet',model.MANet)    \n",
    "\n",
    "\n",
    "\n",
    "net=model.to(device)\n",
    "\n",
    "if device == 'cuda' and args.dataparallel:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "if args.seg_ild:\n",
    "    model_path= f'{current_working_directory}/checkpoint/{name}-Segmentation-{formatted_datetime}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4fdba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 26.913802\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters in the model: {total_params/1e+6}\")\n",
    "run.config.model_no_paras=total_params/1e+6\n",
    "\n",
    "\n",
    "with open(accuracy_file_path, 'a') as f:\n",
    "    f.write(f'\\n ***************************Start******************************** \\n')\n",
    "    for arg in vars(args):\n",
    "        f.write(f\"{arg}: {getattr(args, arg)}  \\n\")\n",
    "    f.write(f\"Total parameters in the model: {total_params/1e+6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab33a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1,alpha2,alpha3,alpha4,alpha5=0.95, 0.1/4, 0.1/8, 0.1/16, 0.1/32\n",
    "\n",
    "class_loss_fn = nn.CrossEntropyLoss()\n",
    "seg_loss_fn   = nn.BCEWithLogitsLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437bf44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [args.seg_size, args.seg_size]\n",
    "new_size = [size // 2 for size in input_size]\n",
    "\n",
    "def train_epoch_Seg(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    \n",
    "    global new_size\n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'\\n Epoch:{epoch}\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "    net.train()\n",
    "    if freeze_all and not(cls_ild):\n",
    "    \n",
    "        for name, module in net.module.features.named_modules():\n",
    "            if isinstance(module, (nn.BatchNorm2d, nn.Dropout)):\n",
    "                module.eval()\n",
    "                \n",
    "                \n",
    "    train_loss = 0\n",
    "    train_ce_loss=0\n",
    "    averageIoU=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    for batch_idx, (inputs, targets, masks) in enumerate(trainloader):\n",
    "        inputs, targets, masks = inputs.to(device), targets.to(device), masks.to(device)\n",
    "\n",
    "        masks=masks.unsqueeze(1)\n",
    "        masks = F.interpolate(masks, size=new_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "     \n",
    "        ce_loss_,se_loss_,mse_loss_=0,0,0\n",
    "        \n",
    "        \n",
    "        if mask_guided:\n",
    "            outputs = net(inputs, masks) \n",
    "            msks=outputs['mask']\n",
    "            fg_att=outputs['fg_att']\n",
    "            mse_loss_ = mse(fg_att,msks)\n",
    "\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        if seg_ild:\n",
    "            Final_seg=outputs['Final_seg']\n",
    "            se_loss_ = seg_loss_fn(Final_seg,masks)\n",
    "\n",
    "            preds = torch.sigmoid(Final_seg)\n",
    "            preds = (preds > 0.5).float()\n",
    "\n",
    "            iou = iou_binary(preds, masks)\n",
    "            averageIoU+=iou\n",
    "\n",
    "            if args.fsds:\n",
    "\n",
    "\n",
    "                lvl_2_loss = seg_loss_fn(outputs['decoder_layer_2'],masks)\n",
    "\n",
    "                fsds_size = [size // 2 for size in new_size]\n",
    "                masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                lvl_3_loss = seg_loss_fn(outputs['decoder_layer_3'],masks)\n",
    "\n",
    "                fsds_size = [size // 2 for size in fsds_size]\n",
    "                masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                lvl_4_loss = seg_loss_fn(outputs['decoder_layer_4'],masks)\n",
    "\n",
    "                fsds_size = [size // 2 for size in fsds_size]\n",
    "                masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                lvl_5_loss = seg_loss_fn(outputs['decoder_layer_5'],masks)\n",
    "                se_loss_= alpha1*se_loss_+ alpha2*lvl_2_loss+ alpha3*lvl_3_loss+ alpha4*lvl_4_loss + alpha5*lvl_5_loss\n",
    "\n",
    "\n",
    "\n",
    "            out=outputs['out']\n",
    "            ce_loss_ = class_loss_fn(out, targets)\n",
    "\n",
    "            loss =  seg_ild*se_loss_ + cls_ild*ce_loss_+ mask_guided*0.1*mse_loss_\n",
    "\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_ce_loss+=ce_loss_.item()\n",
    "\n",
    "        \n",
    "        _, predicted = out.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        Final_seg,masks,inputs,targets,outputs=None,None,None,None,None\n",
    "    \n",
    "    train_ce_loss/=(batch_idx+1)\n",
    "    train_loss/=(batch_idx+1)\n",
    "        \n",
    "    averageIoU=averageIoU*100/(batch_idx+1)\n",
    "    accuracy=100.*correct/total\n",
    "    \n",
    "    print(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {train_ce_loss:.4f}| Total Loss: {train_loss:.4f}| IoU :{averageIoU:.4f}')\n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {train_ce_loss:.4f}| Total Loss: {train_loss:.4f}| IoU :{averageIoU:.4f}\\n')\n",
    "\n",
    "    \n",
    "    wandb.log({\"Current Training Epoch\": epoch,\n",
    "               \"Training IoU\": averageIoU,\n",
    "               \"Training Accuracy\":accuracy},step=epoch)\n",
    "    train_total_losses.append(train_loss)\n",
    "    train_iou.append(averageIoU)\n",
    "    return averageIoU,accuracy,train_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9deb13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch_Seg(epoch):\n",
    "    net.eval()\n",
    "    global best_iou, best_acc, new_size\n",
    "    test_loss = 0\n",
    "    test_ce_loss = 0\n",
    "    \n",
    "    averageIoU=0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets,masks) in enumerate(testloader):\n",
    "            inputs, targets,masks= inputs.to(device), targets.to(device),masks.to(device)\n",
    "            \n",
    "            \n",
    "            masks=masks.unsqueeze(1)\n",
    "            masks = F.interpolate(masks, size=new_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "\n",
    "            ce_loss_,se_loss_,mse_loss_=0,0,0\n",
    "\n",
    "            if mask_guided:\n",
    "                outputs=net(inputs,masks)\n",
    "                msks=outputs['mask']\n",
    "                fg_att=outputs['fg_att']\n",
    "                mse_loss_ = mse(fg_att,msks)\n",
    "            else:\n",
    "                outputs=net(inputs)\n",
    "                \n",
    "\n",
    "            if seg_ild:\n",
    "                Final_seg=outputs['Final_seg']\n",
    "                se_loss_ = seg_loss_fn(Final_seg,masks)\n",
    "\n",
    "                preds = torch.sigmoid(Final_seg)\n",
    "                preds = (preds > 0.5).float()\n",
    "\n",
    "                iou = iou_binary(preds, masks)\n",
    "                averageIoU+=iou\n",
    "                \n",
    "                                    \n",
    "                if args.fsds:\n",
    "                    \n",
    "                    \n",
    "                    lvl_2_loss = seg_loss_fn(outputs['decoder_layer_2'],masks)\n",
    "\n",
    "                    fsds_size = [size // 2 for size in new_size]\n",
    "                    masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                    lvl_3_loss = seg_loss_fn(outputs['decoder_layer_3'],masks)\n",
    "\n",
    "                    fsds_size = [size // 2 for size in fsds_size]\n",
    "                    masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                    lvl_4_loss = seg_loss_fn(outputs['decoder_layer_4'],masks)\n",
    "\n",
    "                    fsds_size = [size // 2 for size in fsds_size]\n",
    "                    masks = F.interpolate(masks, size=fsds_size, mode='bilinear', align_corners=False)\n",
    "                    lvl_5_loss = seg_loss_fn(outputs['decoder_layer_5'],masks)\n",
    "                    se_loss_= alpha1*se_loss_+ alpha2*lvl_2_loss+ alpha3*lvl_3_loss+ alpha4*lvl_4_loss + alpha5*lvl_5_loss\n",
    "\n",
    "\n",
    "            out=outputs['out']\n",
    "            ce_loss_ = class_loss_fn(out, targets)\n",
    "\n",
    "            loss =  seg_ild*se_loss_ + cls_ild*ce_loss_+ mask_guided*0.1*mse_loss_\n",
    "\n",
    "\n",
    "\n",
    "            test_ce_loss+=ce_loss_.item()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _, predicted = out.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            segs,masks,inputs,targets,outputs=None,None,None,None,None\n",
    "\n",
    "    averageIoU=averageIoU*100/(batch_idx+1)\n",
    "    accuracy=100.*correct/total\n",
    "    \n",
    "\n",
    "    if averageIoU>best_iou:\n",
    "        best_iou=averageIoU\n",
    "        \n",
    "    if accuracy>best_acc:\n",
    "        best_acc=accuracy\n",
    "    \n",
    "    test_ce_loss/=(batch_idx+1)\n",
    "    test_loss/=(batch_idx+1)\n",
    "    \n",
    "    test_total_losses.append(test_loss)\n",
    "    test_iou.append(averageIoU)\n",
    "    \n",
    "    print(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {test_ce_loss:.4f}|  Total Loss: {test_loss:.4f}| IoU :{averageIoU:.4f}')\n",
    "    print('cur_iou:{0},best_iou:{1}:'.format(averageIoU,best_iou))\n",
    "    print('curr_Acc:{0},best_Acc:{1}:'.format(accuracy,best_acc))\n",
    "    \n",
    "    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Acc: {accuracy:.3f}% ({correct}/{total})| CE: {test_ce_loss:.4f}| Total Loss: {test_loss:.4f}| IoU :{averageIoU:.4f}\\n')\n",
    "        f.write(f'cur_iou:{averageIoU},best_iou:{best_iou}\\n')\n",
    "        f.write(f'curr_Acc:{accuracy},best_Acc:{best_acc}\\n')\n",
    "        \n",
    "    wandb.log({\"Testing Epoch\": epoch,\n",
    "               \"Current Testing IoU\": averageIoU,\n",
    "               \"Overall Best Testing Iou\":best_iou,\n",
    "               \"Current Testing Accuracy\":accuracy,\n",
    "               \"Best Testing Accuracy\": best_acc},step=epoch)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    return averageIoU,accuracy,test_ce_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56b8727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_seg_performance(iou,curr_test_iou,test_acc,net,epoch,end,start,model_path,accuracy_file_path):\n",
    "    global train_best_iou,best_test_iou,Best_test_acc_BOT\n",
    "    if iou>train_best_iou:\n",
    "        print('Saving..')\n",
    "        train_best_iou=iou\n",
    "        best_test_iou=curr_test_iou\n",
    "        Best_test_acc_BOT=test_acc\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net_wrap = net.module\n",
    "        else:\n",
    "            net_wrap=net\n",
    "        state = {'net': net_wrap.state_dict(), 'test_iou': best_test_iou, 'Test_acc': test_acc , 'epoch': epoch,}\n",
    "        torch.save(state, model_path)\n",
    "    print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "    print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "    print(f'Time Elapsed:{end-start}\\n')    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "        f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "        f.write(f'Time Elapsed:{end-start}\\n')\n",
    "        \n",
    "    wandb.log({\"Best Testing IoU BOT\": best_test_iou,\n",
    "               \"Best Testing Accuracy BOT\":Best_test_acc_BOT,\n",
    "               \"Elapsed Time\":end-start},step=epoch)\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "def check_class_performance(train_ce_loss, test_acc, curr_test_iou,net, epoch, model_path, accuracy_file_path, start, end):    \n",
    "    global min_loss, Best_test_acc_BOT, best_test_iou\n",
    "    if train_ce_loss<min_loss:\n",
    "        print('Saving..')\n",
    "        min_loss=train_ce_loss\n",
    "        Best_test_acc_BOT=test_acc\n",
    "        best_test_iou=curr_test_iou\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net_nwrap = net.module\n",
    "        else:\n",
    "            net_nwrap=net\n",
    "        state = {'net': net_nwrap.state_dict(), 'test_iou': curr_test_iou, 'Test_acc': test_acc, 'epoch': epoch,}\n",
    "        torch.save(state, model_path)\n",
    "    \n",
    "    print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "    print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "    print(f'Time Elapsed:{end-start}\\n')    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "        f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "        f.write(f'Time Elapsed:{end-start}\\n')\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "def check_performance(iou, curr_test_iou, train_ce_loss, test_acc, net, epoch, model_path, accuracy_file_path, start, end):\n",
    "    global train_best_iou, best_test_iou, Best_test_acc_BOT, min_loss\n",
    "    updated = False\n",
    "    # Check for segment performance improvement\n",
    "    if iou > train_best_iou:\n",
    "        print('Saving based on segment performance improvement..')\n",
    "        train_best_iou = iou\n",
    "        best_test_iou = curr_test_iou\n",
    "        Best_test_acc_BOT = test_acc\n",
    "        updated = True\n",
    "\n",
    "    # Check for class performance improvement\n",
    "    if train_ce_loss < min_loss:\n",
    "        print('Saving based on class performance improvement..')\n",
    "        min_loss = train_ce_loss\n",
    "        Best_test_acc_BOT = test_acc\n",
    "        best_test_iou = curr_test_iou\n",
    "        updated = True\n",
    "\n",
    "    # Save the model if there was an update\n",
    "    if updated:\n",
    "        if isinstance(net, torch.nn.DataParallel):\n",
    "            net_nwrap = net.module\n",
    "        else:\n",
    "            net_nwrap = net\n",
    "        state = {'net': net_nwrap.state_dict(),'test_iou': curr_test_iou,'test_acc': test_acc,'epoch': epoch,}\n",
    "        torch.save(state, model_path)\n",
    "        \n",
    "    print(f'Best Testing IoU Based On the Training:{best_test_iou}')\n",
    "    print(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}')\n",
    "    print(f'Time Elapsed:{end-start}\\n')    \n",
    "    with open(accuracy_file_path, 'a') as f:\n",
    "        f.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "        f.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "        f.write(f'Time Elapsed:{end-start}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a1f1e7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_iou=0\n",
    "best_acc=0\n",
    "\n",
    "train_best_iou=0\n",
    "best_test_iou=0\n",
    "\n",
    "min_loss=1e10\n",
    "test_acc=0\n",
    "Best_test_acc_BOT=0\n",
    "\n",
    "def evaluate_model(model=net):\n",
    "    for epoch in range(start_epoch, args.max_epoch):\n",
    "        start = time.time()\n",
    "        iou,train_acc,train_ce_loss= train_epoch_Seg(epoch)\n",
    "        curr_test_iou,test_acc,test_ce_loss=test_epoch_Seg(epoch)\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "\n",
    "        if seg_ild and not (cls_ild):\n",
    "            check_seg_performance(iou,curr_test_iou,test_acc,net,epoch,end,start,model_path,accuracy_file_path) \n",
    "        elif not(seg_ild) and cls_ild:\n",
    "            check_class_performance(train_ce_loss, test_acc, curr_test_iou,net, epoch, model_path, accuracy_file_path, start, end)\n",
    "        else:\n",
    "            check_performance(iou, curr_test_iou, train_ce_loss, test_acc, net, epoch, model_path, accuracy_file_path, start, end)\n",
    "        nni.report_intermediate_result(best_test_iou)\n",
    "\n",
    "    nni.report_intermediate_result(best_test_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac84b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.nas.evaluator import FunctionalEvaluator\n",
    "evaluator = FunctionalEvaluator(evaluate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "520d42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-17 16:17:58] \u001b[32mConfig is not provided. Will try to infer.\u001b[0m\n",
      "[2024-01-17 16:17:58] \u001b[32mUsing execution engine based on training service. Trial concurrency is set to 1.\u001b[0m\n",
      "[2024-01-17 16:17:58] \u001b[32mUsing simplified model format.\u001b[0m\n",
      "[2024-01-17 16:17:58] \u001b[32mUsing local training service.\u001b[0m\n",
      "[2024-01-17 16:17:58] \u001b[33mWARNING: GPU found but will not be used. Please set `experiment.config.trial_gpu_number` to the number of GPUs you want to use for each trial.\u001b[0m\n",
      "[2024-01-17 16:17:58] \u001b[32mCreating experiment, Experiment ID: \u001b[36mfb9gx3n7\u001b[0m\n",
      "[2024-01-17 16:17:58] \u001b[32mStarting web server...\u001b[0m\n",
      "[2024-01-17 16:17:59] \u001b[33mWARNING: Timeout, retry...\u001b[0m\n",
      "[2024-01-17 16:18:00] \u001b[33mWARNING: Timeout, retry...\u001b[0m\n",
      "[2024-01-17 16:18:01] \u001b[32mSetting up...\u001b[0m\n",
      "[2024-01-17 16:18:02] \u001b[32mWeb portal URLs: \u001b[36mhttp://127.0.0.1:8081 http://10.14.1.13:8081 http://172.17.0.1:8081\u001b[0m\n",
      "[2024-01-17 16:18:02] \u001b[32mSuccessfully update searchSpace.\u001b[0m\n",
      "[2024-01-17 16:18:02] \u001b[32mCheckpoint saved to /home/pupil/rmf3mc/nni-experiments/fb9gx3n7/checkpoint.\u001b[0m\n",
      "[2024-01-17 16:18:02] \u001b[32mExperiment initialized successfully. Starting exploration strategy...\u001b[0m\n",
      "[2024-01-17 16:18:02] \u001b[31mERROR: Strategy failed to execute.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pickle too large when trying to dump <function evaluate_model at 0x7f53f0be3be0>. Please try to raise pickle_size_limit if you insist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/common/serializer.py:831\u001b[0m, in \u001b[0;36mget_hybrid_cls_or_func_name\u001b[0;34m(cls_or_func, pickle_size_limit)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 831\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[43m_get_cls_or_func_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_or_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# import success, use a path format\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/common/serializer.py:810\u001b[0m, in \u001b[0;36m_get_cls_or_func_name\u001b[0;34m(cls_or_func)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot use a path to identify something from __main__.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    811\u001b[0m full_name \u001b[38;5;241m=\u001b[39m module_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m cls_or_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot use a path to identify something from __main__.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m exp\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrial_gpu_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m exp\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraining_service\u001b[38;5;241m.\u001b[39muse_active_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8081\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/experiment/experiment.py:236\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self, port, wait_completion, debug)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8080\u001b[39m, wait_completion: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, debug: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    Run the experiment.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    Otherwise, return ``True`` when experiment done; or return ``False`` when experiment failed.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_completion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/experiment/experiment.py:205\u001b[0m, in \u001b[0;36mExperiment._run_impl\u001b[0;34m(self, port, wait_completion, debug)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, port: \u001b[38;5;28mint\u001b[39m, wait_completion: \u001b[38;5;28mbool\u001b[39m, debug: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m wait_completion:\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_completion()\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/nas/experiment/experiment.py:270\u001b[0m, in \u001b[0;36mNasExperiment.start\u001b[0;34m(self, port, debug, run_mode)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_without_nni_manager()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_engine_and_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/nas/experiment/experiment.py:230\u001b[0m, in \u001b[0;36mNasExperiment._start_engine_and_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_checkpoint()\n\u001b[1;32m    228\u001b[0m _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExperiment initialized successfully. Starting exploration strategy...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/nas/strategy/base.py:170\u001b[0m, in \u001b[0;36mStrategy.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_status \u001b[38;5;241m=\u001b[39m StrategyStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Explore the model space.\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Strategy doesn't wait for the models it submitted.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrategy has successfully finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/nas/strategy/bruteforce.py:223\u001b[0m, in \u001b[0;36mRandom._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_resource():\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/nas/execution/training_service.py:172\u001b[0m, in \u001b[0;36mTrainingServiceExecutionEngine.submit_models\u001b[0;34m(self, *models)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, GraphModelSpace):\n\u001b[1;32m    170\u001b[0m     placement \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mexport_placement_constraint()\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_channel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameter_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplacement_constraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplacement\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m model\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m ModelStatus\u001b[38;5;241m.\u001b[39mTraining\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/runtime/tuner_command_channel/channel.py:144\u001b[0m, in \u001b[0;36mTunerCommandChannel.send_trial\u001b[0;34m(self, parameter_id, parameters, parameter_source, parameter_index, placement_constraint)\u001b[0m\n\u001b[1;32m    141\u001b[0m     trial_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplacement_constraint\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m placement_constraint\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     send_payload \u001b[38;5;241m=\u001b[39m \u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPICKLE_SIZE_LIMIT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PayloadTooLarge:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSerialization failed when trying to dump the model because payload too large (larger than 64 KB). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is usually caused by pickling large objects (like datasets) by mistake. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the full error traceback for details and https://nni.readthedocs.io/en/stable/NAS/Serialization.html \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor how to resolve such issue. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    151\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/common/serializer.py:372\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, use_trace, pickle_size_limit, allow_nan, **json_tricks_kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m json_tricks_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you meant to compress the dumped payload, please use `dump_bytes`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 372\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_dump\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpickle_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjson_tricks_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;28mstr\u001b[39m, result)\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/common/serializer.py:424\u001b[0m, in \u001b[0;36m_dump\u001b[0;34m(obj, fp, use_trace, pickle_size_limit, allow_nan, **json_tricks_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_tricks\u001b[38;5;241m.\u001b[39mdump(obj, fp, obj_encoders\u001b[38;5;241m=\u001b[39mencoders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjson_tricks_kwargs)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_tricks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_encoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjson_tricks_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/json_tricks/nonp.py:125\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, sort_keys, cls, obj_encoders, extra_obj_encoders, primitives, compression, allow_nan, conv_str_byte, fallback_encoders, properties, **jsonkwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \t\u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m TricksEncoder\n\u001b[1;32m    122\u001b[0m combined_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(sort_keys\u001b[38;5;241m=\u001b[39msort_keys, obj_encoders\u001b[38;5;241m=\u001b[39mencoders, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    123\u001b[0m \tprimitives\u001b[38;5;241m=\u001b[39mprimitives, fallback_encoders\u001b[38;5;241m=\u001b[39mfallback_encoders,\n\u001b[1;32m    124\u001b[0m   \tproperties\u001b[38;5;241m=\u001b[39mproperties, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjsonkwargs)\n\u001b[0;32m--> 125\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_py3 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(txt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    127\u001b[0m \ttxt \u001b[38;5;241m=\u001b[39m unicode(txt, ENCODING)\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/json_tricks/encoders.py:76\u001b[0m, in \u001b[0;36mTricksEncoder.default\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m prev_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mid\u001b[39m(obj)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj_encoders:\n\u001b[0;32m---> 76\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimitives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimitives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_changed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprev_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(obj) \u001b[38;5;241m==\u001b[39m prev_id:\n\u001b[1;32m     78\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{0:}\u001b[39;00m\u001b[38;5;124m could not be encoded by \u001b[39m\u001b[38;5;132;01m{1:}\u001b[39;00m\u001b[38;5;124m using encoders [\u001b[39m\u001b[38;5;132;01m{2:s}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     79\u001b[0m \t\t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou can add an encoders for this type using `extra_obj_encoders`. If you want to \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     80\u001b[0m \t\t\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject, consider using `fallback_encoders` like `str` or `lambda o: None`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     81\u001b[0m \t\t\t\u001b[38;5;28mtype\u001b[39m(obj), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(encoder) \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj_encoders)))\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/json_tricks/utils.py:66\u001b[0m, in \u001b[0;36mfiltered_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 66\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/common/serializer.py:858\u001b[0m, in \u001b[0;36m_json_tricks_func_or_cls_encode\u001b[0;34m(cls_or_func, primitives, pickle_size_limit)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cls_or_func, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_function(cls_or_func):\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# not a function or class, continue\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_or_func\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__nni_type__\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mget_hybrid_cls_or_func_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_or_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_size_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m }\n",
      "File \u001b[0;32m~/.conda/envs/UnetCRF2/lib/python3.10/site-packages/nni/common/serializer.py:837\u001b[0m, in \u001b[0;36mget_hybrid_cls_or_func_name\u001b[0;34m(cls_or_func, pickle_size_limit)\u001b[0m\n\u001b[1;32m    835\u001b[0m b \u001b[38;5;241m=\u001b[39m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(cls_or_func)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m>\u001b[39m pickle_size_limit:\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPickle too large when trying to dump \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_or_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    838\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease try to raise pickle_size_limit if you insist.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# fallback to cloudpickle\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(b)\u001b[38;5;241m.\u001b[39mdecode()\n",
      "\u001b[0;31mValueError\u001b[0m: Pickle too large when trying to dump <function evaluate_model at 0x7f53f0be3be0>. Please try to raise pickle_size_limit if you insist."
     ]
    }
   ],
   "source": [
    "from nni.nas.experiment import NasExperiment\n",
    "exp = NasExperiment(net, evaluator, search_strategy)\n",
    "\n",
    "exp.config.trial_gpu_number = 1\n",
    "exp.config.training_service.use_active_gpu = True\n",
    "\n",
    "exp.run(port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()\n",
    "\n",
    "all_results_file =os.path.join(current_working_directory,train_type,args.backbone_class,'all_results_file.txt')\n",
    "\n",
    "\n",
    "\n",
    "from filelock import Timeout, FileLock\n",
    "lock = FileLock(all_results_file[:-4]+'.lock', timeout=120)  # Timeout after two minutes\n",
    "try:\n",
    "    with lock:\n",
    "        with open(all_results_file, 'a') as file:\n",
    "            file.write(f'\\n*************************************************************\\n')\n",
    "            file.write(f\"**{name} 's Testing Accuracies**\\n\")\n",
    "            file.write(f'***Total parameters in the model: {total_params/1e+6}***\\n')            \n",
    "            file.write(f'*****{formatted_datetime} Time*****\\n')\n",
    "            file.write('********Training IOU\\n')\n",
    "            file.write(f'Training IOU:{iou}\\n')\n",
    "            file.write(f'Best Training IOU:{train_best_iou}\\n')\n",
    "            file.write('********Testing IOU\\n')\n",
    "            file.write(f'The overall Very Best Testing IOU :{best_iou}\\n')\n",
    "            file.write(f'Best Testing IoU Based On the Training:{best_test_iou}\\n')\n",
    "            file.write('********\\n')\n",
    "            file.write('Training Acc*****\\n')\n",
    "            file.write(f'Training Loss:{train_ce_loss}\\n')\n",
    "            file.write(f'Best Testing Accuracy Based On the Training:{Best_test_acc_BOT}\\n')\n",
    "            file.write(f'The overall Very Best Testing Accuracy :{best_acc}\\n')\n",
    "            file.write(f'*************************************************************\\n\\n')\n",
    "except Timeout:\n",
    "    print(\"Could not acquire the lock within 120 seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-UnetCRF2] *",
   "language": "python",
   "name": "conda-env-.conda-UnetCRF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
